---
course: Dynamous Agentic Coding
module: 5
lesson: "5.6"
title: "Connecting Our Agents to Obsidian (PIV Loop 2)"
type: lecture
status: raw
date_added: 2026-02-18
date_modified: 2026-02-18
tags:
  - dynamous
  - agentic-coding
  - module-5-planning-system
  - piv-loop
  - obsidian-agent
key_concepts: []
prerequisites: ["5.5"]
related_lessons: ["5.7", "6.1"]
---


So in the last video, we went through the PIV

00:02

loop for the very first time creating our base agent.

00:05

Now I wanna go through the PIV loop for you

00:07

again. This time setting up the connection with our Obsidian

00:11

Vault, connecting it to the Copilot plugin that we'll install

00:14

and configure at the end of this video. And so

00:17

we really get to see the agent in action for

00:19

the first time.

00:21

And so I'm showing you here that we are doing

00:23

micro iterations with each PIV loop, starting with just the

00:27

base agent,

00:28

then just the Obsidian connection. In the next couple of

00:31

modules, when we do the PIV loops there, we'll be

00:33

adding tools 1 at a time into the agent. So

00:36

very organized,

00:37

just super granular, knock it out 1 by 1, build

00:41

up our project with many PIV loops. And the other

00:44

reason that I want to do another PIV loop with

00:46

you here is to set the stage for you, both

00:49

for your exercise coming up and for the next module.

00:52

Because when we get into implementation,

00:54

I'll use the PIV loops there as an opportunity

00:57

to talk about how we evolve our systems. Because since

01:00

I've just been focusing on planning here, we haven't really

01:02

seen anything actually go wrong. And so I want to

01:05

look at what happens when things go wrong, how we

01:08

can evolve our system, and then we'll expand on that

01:11

even more in module 7.

01:13

Alright. So let's get right into our next PIV loop,

01:15

and I want you to get so comfortable

01:18

with this entire process. Before we really start to automate

01:22

things with remote agent decoding and command chaining and all

01:24

those things that are coming up, want you to be

01:26

able to do this in your sleep. That's why I'm

01:29

repeating this quite a few times throughout the next few

01:31

modules.

01:32

And so how do we start our vibe planning? What's

01:35

the first step of our system?

01:37

Yes. That's right. We are going to go and prime

01:40

our coding assistant on our code base. So just a

01:42

simple slash prime. I'm using the generic 1 because we're

01:45

not priming for tools yet. We're not adding tools, but

01:48

we will be doing that pretty soon. Actually, probably right

01:50

after this PIV loop here. So I'll come back once

01:53

it understands our code base.

01:55

Alright. Our prime is complete. And remember, the output is

01:58

formatted here for us to read through and make sure

02:01

that it understands our code base. And things look really

02:04

good. But actually, there's 1 thing I noticed is it's

02:07

thinking that our next step here is to go and

02:09

add some tools to our agent. But it doesn't say

02:12

anything about how we're missing the Obsidian integration and the

02:16

API endpoints for that. And so I did a little

02:19

follow-up here just to kind of add on to the

02:20

priming. This is a good example of how I read

02:23

through its output and realized it didn't quite understand things.

02:27

For some reason, it decided to do the priming process

02:30

in, like, 10 seconds. Like, it was super, super fast.

02:32

It didn't dig as deep as usually it would. There's

02:35

probably an opportunity to refine our priming command there, but

02:38

I'll save that for module 7. So okay. Alright. So

02:42

then I just did a follow-up here. We're not ready

02:44

for ObsidianNet, are we? And so then it figured that

02:47

out. Okay. Good. So now we're on the same page.

02:49

That's the next thing we need to do. Onto the

02:51

rest of our vibe planning. So in the last couple

02:54

of PIV loops, I've done a lot of smaller prompts

02:56

during the vibe planning stage. Like, I want to research

02:59

this, or I wanna consider this, or give me options

03:02

for x y z. And especially if you're not comfortable

03:05

with your code base, you definitely need to do that

03:07

to get to the point where you have all the

03:09

context in a conversation

03:11

to move on to that structured plan. But I also

03:14

wanna use this pivot loop as an opportunity to show

03:16

you that even in the planning stage, you can move

03:19

pretty quick through these iterations when you have a lot

03:21

of context in mind already

03:23

to give to your coding assistant. And so I'm gonna

03:25

go ahead and paste in a longer prompt here and

03:29

explain it really quick. But the main point that I'm

03:31

driving across here is when you have a lot of

03:33

knowledge already, you can speed up the process

03:36

by just dumping all this context, having it consider it,

03:39

and research it. For example, I already know a couple

03:42

of URLs that I want my coding assistant to research.

03:44

The 1 for the Copilot plugin directly and then also

03:47

the open source repo because all Obsidian plugins are open

03:50

source. And so as an experienced engineer, I was curious

03:53

enough to do some research ahead of time. Looking at

03:56

the code, I even identified a specific file in the

03:59

code base that I wanted to analyze so that it

04:01

understands how to do this integration better. So, again, if

04:03

you're newer to coding or working with coding assistance, this

04:06

isn't a requirement. You can have just a longer conversation

04:10

during Vybe planning to get to the point where you've

04:12

identified key things to research, you have these URLs analyzed.

04:15

But

04:16

if I've done some of my own work upfront, I

04:19

can throw in something like this and just have it

04:21

consider all of this together. Also, because I'm a more

04:24

experienced engineer, there's already some questions I have in mind

04:26

that I want to consider

04:28

just to really make sure that this setup works well.

04:31

Because there are very specific ways our API endpoints have

04:34

to be crafted for our Pydantic AI agent to be

04:38

compatible

04:39

with what the Obsidian plug in expects. And so I'm

04:41

asking questions to guide it to get to that point

04:44

where it understands everything.

04:45

And not only does it research these resources, but it

04:48

does it the right way, where it's looking at the

04:50

right things

04:51

to understand, for example, how the content field is structured.

04:54

So I'm getting pretty specific here. There's a lot of

04:57

research going on, but also, that's gonna be a part

05:00

of your exercise coming up where you'll add more research

05:04

steps into your planning command so that a lot of

05:07

this can actually be taken care of as a part

05:09

of your system as well.

05:11

And so there'll still be a place for you to

05:12

manually inject the questions and things you wanted to consider,

05:15

but also maybe your prompts don't have to be this

05:18

long. So I'm gonna go ahead and send this in,

05:20

and it's gonna take quite a bit here to research

05:22

and consider everything. And so it'll come back now with

05:25

a report for how we're going to build this OpenAI

05:29

API compatibility for the Obsidian agent, and this could also

05:33

be included as our on demand context as well going

05:35

forward. So we could actually add this to our layer

05:38

1 planning.

05:39

Very good. The agent came back. It created the report

05:42

that I'll show you in a second, and it even

05:44

understands the specific endpoint we have to build now as

05:47

well. And so, yeah, definitely validate your reports after you

05:50

create them. And you don't have to generate reports either.

05:53

You can just keep within the context of the conversation.

05:56

But I'm just showing you that you also can generate

05:58

these documents throughout your vibe planning that maybe you want

06:01

to reference later, like I said, as a part of

06:03

your on demand context. And so this is just the

06:06

summary for how the Copilot plug in works. It takes

06:09

advantage of OpenAI compatible APIs. That's how we're gonna connect

06:13

our Pydantic AI agent in a little bit as well.

06:16

And so it understands the structure. It's calling out specific

06:18

parts of the code since it's an open source repo

06:20

that it researched.

06:22

1 really good example of something that we've seen Rasmus

06:25

and I have seen it mess up on before without

06:27

this research is the endpoint is actually tacked on automatically

06:32

to the URL that we specify to reach our agent.

06:35

And so we don't want to, in the Copilot settings,

06:39

give the full URL all the way to the complete

06:42

endpoint because otherwise it's gonna be duplicated like this. And

06:45

so, yeah, it's just really good to have this research

06:47

and reference it when it is working with this integration.

06:51

And so with that, I'm gonna move on to another

06:53

research request here because we understand

06:56

our Obsidian Copilot plugin. But now we need to do

06:59

some research for how we make this work with Pydantic

07:02

AI specifically and working with their output formatting. So, again,

07:05

providing quite a bit of context here just because I'm

07:08

very experienced with Pydantic AI.

07:11

And so, for example, I know that the way to

07:14

stream text from an agent with Pydantic AI is to

07:17

use their dot iter function. There are some other functions

07:20

out there for streaming, but this is the best 1.

07:23

And if you don't know that ahead of time like

07:25

I do, you'll get there if you're researching with the

07:27

coding assistant. But I know this already, so I'm gonna

07:30

give it some sources from the documentation for it to

07:33

analyze,

07:34

talk about some of my opinions, and then some of

07:36

the questions that I wanted to consider. So guiding it

07:39

as much as I can. Like I always say, reducing

07:42

the number of assumptions the coding assistant is making by

07:44

injecting my own knowledge and guiding it with specific questions.

07:48

And so I'm not gonna cover this in an insane

07:50

amount of detail here. I mean, half of this really

07:52

is just the report format. So exactly how I wanted

07:54

to output something, which, by the way, that should also

07:57

kind of speak to how this probably should be a

08:00

command, at least for part of it. Right? Like, we

08:02

have our input,

08:04

we have our process, and we have our output.

08:07

You could definitely have this as a part of your

08:10

planning command, and so that will lead into the next

08:13

exercise here. So I'm gonna send off this request, come

08:15

back with something very similar, where it'll create another report,

08:18

this time for how we use the Pydantic AI side

08:21

of things.

08:22

Okay. We now have our second report created. And the

08:26

first time it made it, it was 1500 lines long,

08:29

like, way, way too long. So I did another follow-up

08:31

prompt to make it more concise, and that's just a

08:34

little golden nugget for you going into the next exercise.

08:36

When we're adding a research process to our planning command,

08:40

you want to make sure that it is concise but

08:43

comprehensive. That's the wording that I always love to use

08:45

for coding assistance. Make sure you're still covering everything, but

08:48

you gotta be concise. 300 lines here is a lot

08:51

better than the 1500 we had at first. And this

08:54

is looking really good. It understands Paradigm AI, how to

08:56

use the dot inner function, how the chunks are streamed

08:59

out. Very, very nice. And so I could iterate more

09:02

in the Vybe planning if I want, but I'm gonna

09:04

go right into creating our structured plan now. So go

09:07

ahead and paste in this prompt using the plan template

09:09

command just like our last iteration of the PIV loop.

09:12

And I'm calling out that I want it to reference

09:14

the files I created in the report folder. So making

09:17

sure it understands the iter function, how to handle the

09:20

OpenEye API compatibility.

09:22

We also need cores as well, which I'm just injecting

09:24

a bit more of my knowledge than I know upfront

09:26

from doing research into the plugin.

09:28

And then another thing, I'm pulling in some of our

09:30

on demand layer 1 planning context here. So I'm pulling

09:33

in the vertical slice architecture. This is gonna be very

09:36

important as we're creating more API endpoints here that we're

09:39

setting up the code base with the structure that we

09:42

have documented in this on demand context. So very good.

09:47

And then at the very end here, I'm also creating

09:50

some more on demand context. So I'm creating a new

09:52

markdown file in our reference folder where we have all

09:55

of our other layer 1 planning with user instructions on

09:59

how to set up the agent in the Obsidian Copilot

10:02

plugin. So we're gonna do that after we finish our

10:05

implementation here, but we don't even have to figure it

10:07

out ourselves. As a part of this process of planning

10:10

things, it's even going to document for us how we

10:13

can set things up. So going forward, we can also

10:16

just ask the coding assistant

10:18

how to set this up, like, for a new user

10:20

and it has that context as well. Very beautiful. So

10:23

I'm gonna go ahead and send this in, come back

10:25

once I have the structured plan. Okay. Our plan is

10:28

created and so let's just do a little bit of

10:30

validation here. I looked at it off camera already. 1

10:33

thing I noticed is it is a little too long,

10:35

over 800 lines when remember, we evolved our system to

10:39

a very small extent. I just gave you a simple

10:41

example showing how we're adding, like, okay. Keep it between

10:44

507

10:45

hundred lines long in our plan template command. So it

10:47

didn't quite listen, but you can tell it tried because

10:49

at least it isn't like 1500 lines like the first

10:52

time we generate a structure plan. And I read through

10:55

this all already, and it looks pretty good overall.

10:59

A couple of things I wanna edit really quickly. Going

11:01

down to the testing here towards the bottom, it definitely

11:05

is trying to do more testing than I really need

11:07

right now. Like, we got the unit tests, we have

11:09

the linting, we have the integration tests. We'll get into

11:12

this a lot more in module 7. I don't really

11:15

need it to do the manual validation

11:17

and the additional validation. So I'm gonna go and delete

11:20

these right now. And then also for the trade offs

11:24

and future enhancements, I don't really care about that. So,

11:27

yeah, just keeping it a bit more concise. Okay. So

11:29

we're down to 758

11:31

lines. It's definitely on the longer end for an implementation,

11:34

but what we're doing here is going to be,

11:37

definitely a more complex pivot loop than the last 1.

11:39

So I think this is a reasonable

11:41

structure plan. So with that, we can now get into

11:44

executing it. So for our implementation,

11:47

let's clear our conversation starting with a blank slate because

11:51

remember, we have all the context we need in our

11:54

structure plan. That is the beauty of it now. And

11:56

you can always do a slash prime if you want.

11:58

Sometimes you wanna do that again before execution.

12:01

It just depends on if you look through the plan

12:03

and feel like it needs to understand the code base

12:05

first. But in this case, I'm pretty confident that it

12:08

knows what to touch, where to look based on the

12:11

plan because the documentation section was very well put together,

12:14

and I validated that before getting to where we are

12:17

now. So let's go ahead and send in our prompt.

12:20

So I wanted to read the structure plan and fully

12:23

understand it, go through all the references, make sure it

12:26

understands existing patterns in our code base. And then also,

12:30

1 other thing that I wanted to add here is

12:32

that I wanted to actually do end to end testing,

12:34

like run the application.

12:35

I already have my anthropic API key added to the

12:38

dot e n b. So it's something I just kinda

12:39

decided last minute. So it isn't in the structure plan

12:42

right now, but we can still add in a little

12:44

bit of context when we get into the execution if

12:46

we want. So just showing a simple example of that.

12:49

And by the way, this command

12:51

looks very similar to the 1 we used in the

12:53

last PIV loop going into execution.

12:56

This is a very simple version of what we can

12:59

create as our execute command as the implementation part of

13:03

our system. So we'll get into that in the next

13:06

module. I'm manually prompting like this right now because we're

13:09

just focused on systems for planning in this module. So

13:13

alright. I'll go ahead and send this in.

13:15

It is going to read the structure plan and execute

13:19

it on it task by task.

13:21

And boom. There we go. About 20 minutes later, we

13:24

have our full structured plan implemented.

13:27

And it did all of the validation and everything that

13:30

we asked it to do, covering that a lot more

13:32

in modules 6 and 7, especially 7. So, yeah, with

13:35

that, let's go ahead and just do some manual testing.

13:38

Now I wanna show you this in action. We have

13:40

our agent connected into Obsidian. And so I'll do a

13:43

first test with you and hopefully,

13:45

because of our systems, because of all of our planning,

13:48

it knocked this out in the first try. And also

13:50

thanks to the fact that we are doing a pretty

13:52

small iterations in each PIV loop. And so we have

13:55

our guide as well that I'll link to in the

13:58

description, once I refine this for how we can get

14:01

everything up and running and have it configured within our

14:03

Obsidian Copilot plugin. And so first thing in my terminal

14:07

here, I have my environment variables already set up. I

14:09

got my dependencies

14:11

installed and so now I'm just going to run our

14:13

application specifically on port 80 97. And the port that

14:17

you pick is very important by the way. The default

14:20

1 is 8 1 2 3 but I'm just changing

14:22

it up because I already have something running on that

14:24

port. So alright. There we go. Our application is up

14:27

and running. Now we can go into Obsidian and get

14:29

the Copilot plugin installed and configured.

14:32

So every plugin in Obsidian is completely open source and

14:35

very easy to to install. All we have to do

14:37

is go to the settings in the bottom left,

14:40

go to community plugins. You'll have to turn it on

14:43

if you haven't for the first time, then you can

14:46

browse the list of community plugins available. We can just

14:48

simply search for Copilot. It

14:51

is literally just called Copilot.

14:53

So install this. It's the 1 that has around 850000

14:56

installs at the time of recording here. So get that

14:59

installed and then you just have to toggle it on

15:02

in the install plugin list right here.

15:05

Then once you have that toggled on, you'll see the

15:08

Copilot option

15:09

in the left hand bar here in the settings.

15:12

This is where we'll set up the connection to our

15:14

custom Pidentity AI agent that we have running in the

15:17

terminal now. So you do have to have this running

15:19

first, then we do these things in the Copilot plug

15:22

in. So first, go to the model tab here. And

15:25

this is a really neat plug in. Before you even

15:27

create any custom agents, we can just plug any large

15:29

language model we want into Obsidian. It's not gonna have

15:32

all the same fancy tools that we're gonna build the

15:35

next couple of modules, but that's always pretty neat as

15:37

well. But, yeah, a lot more power with something custom

15:40

like like we'll add right now. And so scroll down

15:42

to where you see add a custom model. And then

15:45

for the model name, we can do whatever we want.

15:47

Like, I'll just say Patty, And this is kind of

15:49

like the model ID. And then the display name is

15:52

what we're going to see in the user interface itself.

15:54

And so I'll do capital p Patty there. And then

15:57

for the provider, we want OpenAI format. That's a k

16:00

a OpenAI API compatibility.

16:03

And then for our base URL,

16:05

that's going to be just based on the port that

16:07

we're running on here. And so it's running on HTTP

16:11

local host 80 97.

16:13

And then we want to tack on the v 1

16:15

as well. And then remember that the completion part of

16:18

the URL is tacked on automatically, so we don't specify

16:21

that here. So it's HTTP.

16:23

It's not HTTPS because it's just running right now in

16:26

local host. We don't have any security set up yet,

16:28

but we will get there. 80 97

16:31

slash v 1. There we go. And then for the

16:33

API key, it's required to put something here but we

16:36

don't have it set up yet where we're validating a

16:38

key. We'll add that in later but this is just

16:41

running locally right now so we don't have to worry

16:43

about security yet. And then just make sure that you

16:45

toggle enable course. For all of our custom agents, we

16:47

need that. And then very cool, we can also do

16:50

a verification here. So you can see that the last

16:53

line of our logs is application startup complete. If I

16:56

click on verify, take a look at that, it sends

16:59

a request to the chat completions endpoint,

17:02

v 1 chat completions. We have a 200 okay, so

17:05

that was successful. And then you might have missed it

17:07

there, but in the top left or top right in

17:09

Obsidian, it said that the verification was successful. Cool. So

17:12

now I can click on add model,

17:14

and now is the real moment of truth.

17:17

Within the UI now, can we talk to Patty

17:20

within

17:21

Obsidian? So I'm gonna click on the expand for our

17:24

right hand bar here. This is how we get access

17:26

to the Copilot in our UI. And then for the

17:28

model, I have it defaulted to another agent that I

17:31

built in a workshop in DynaMask, but I will switch

17:33

it to Patty now. Alright. Moment of truth. Very simply,

17:36

I'm just going to say hi and make sure we

17:38

can get a response from Patty. Alright. In the in

17:40

the terminal, I see that we got a successful message

17:43

sent and yes. Awesome.

17:45

I'm Patty, your Obsidian Assistant.

17:48

How can I assist you today? Alright. Looking really, really

17:51

good. And it seems like there's a tiny issue here

17:54

where the first token was not included.

17:57

So probably something for us to fix. We'll address that

18:00

in module 7 when I talk about validation. I actually

18:03

wanna leave this issue here

18:05

because I wanna come back and think about how we

18:07

can evolve our system and provide better context to avoid

18:10

this kind of thing. But overall, this is working really

18:12

good. So this is actually perfect. A little issue because

18:15

you can't expect coding assistants to nail things 100 percent

18:18

of the time, but we are obviously very, very close,

18:21

like 99 percent of the way through this implementation in

18:24

a single shot, thanks to our planning and iterating through

18:27

our PIV loops. And then I can say, what did

18:29

I just say? Just to make sure that our conversation

18:32

history is working. You said, hi. Yep. And again, it

18:34

cut off that first token, but otherwise, it is working

18:37

perfectly.

18:38

So there you go. That is the first couple of

18:40

PIV loops. That is systems for planning. And so at

18:44

this point, we know how to create our structure plans.

18:47

We have our priming down. We know what it looks

18:49

like to leverage this to go into implementation.

18:52

And so now for your exercise coming up in the

18:55

next video, your job is going to be to expand

18:59

our planning command to include a lot of the researching

19:02

that we've been doing manually in the Vibe planning so

19:04

far. Not that you'll never do any researching at all

19:07

in Vibe planning, but we want to move as much

19:10

of that into the command as possible so that we

19:12

are systematizing things more and typing less. And so I'll

19:15

see you in the next video for that exercise.