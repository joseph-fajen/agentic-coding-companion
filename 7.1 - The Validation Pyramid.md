---
course: Dynamous Agentic Coding
module: 7
lesson: "7.1"
title: "The Validation Pyramid"
type: lecture
status: raw
date_added: 2026-02-18
date_modified: 2026-02-18
tags:
  - dynamous
  - agentic-coding
  - module-7-validation
  - validation-pyramid
  - testing
  - code-review
key_concepts: []
prerequisites: ["6.2"]
related_lessons: ["7.2", "7.3"]
---

Welcome to module 7 of the DynaMed agentic coding course,

00:04

systems for validation.

00:06

We are at the end of our PIV loop now.

00:09

And in the last few PIV loops we've done in

00:11

the last couple of modules, I kept telling you that

00:13

we're skipping validation for the most part because I wanna

00:16

cover the systems for that later, and that is what

00:19

we are doing now. And just like with planning, we

00:22

very much want to be a part of the process

00:24

with validation. It is our role just as much as

00:27

it is the coding assistance.

00:29

And so there are quite a few commands

00:31

that I'm going to show you how to build and

00:33

use in this module

00:35

to create our system for validation and even 2 exercises

00:38

that I have for you to build your own commands.

00:41

So it's gonna be really engaging, and by the end

00:43

of this, you're going to be well on your way.

00:46

And not just with your system for validation, but for

00:49

your system for the entire PIV loop because we have

00:52

gotten through everything at this point.

00:54

And speaking of that, let's talk about all of the

00:56

commands that we have now. Because as we've been going

00:59

through modules 4 through 6 together and as I've been

01:02

prepping this module, I've been curating all of the commands

01:05

together for you because it really is the core of

01:08

our systems.

01:09

And so please use this as a resource. I'll have

01:12

it linked in the description below. I'm gonna be linking

01:14

to this quite a bit throughout these modules here because

01:18

I want you to take inspiration from these as you

01:21

create the commands for your own system. And I do

01:23

encourage you to create your own commands. I mean, you

01:26

can just take these straight up, but I want you

01:28

to customize things to your projects. Plus, some of these

01:32

are a bit opinionated

01:33

towards the Obsidian AI agent because generally, as you evolve

01:36

your system, it does get more opinionated over time. So

01:39

you start to deviate from generic examples like this or

01:42

what you might have as a starting point. So, yeah,

01:45

we have the core PIV loop commands for priming, planning,

01:47

executing.

01:48

We have the GitHub bug fix commands that I showed

01:51

in that alternative workflow in module 4. Very, very useful.

01:54

We have the ones that don't belong in any specific

01:56

category like committing and creating our PRD, and then we

01:59

have the commands that I'll be covering in this module

02:02

specifically. There's quite a bit here because we very much

02:05

wanna be a part of the process. And then there's

02:08

also the very exciting system review command that we'll cover

02:11

at the end of this module.

02:13

Creating a process for us to go back to the

02:16

planning after implementation,

02:18

find discrepancies

02:19

between them that give us opportunities to evolve our system.

02:22

So we are systematizing

02:24

the system improvement. I know it's very meta, but it

02:26

is super powerful. And so that's what we got coming

02:29

up for this module. We're gonna talk about reviewing

02:32

code changes, then talk about reviewing our system and exercises

02:36

for both. And so starting with this video here, I

02:39

want to cover the 5 level validation pyramid with you.

02:43

And I wanna start with the validation pyramid because it

02:46

really is the core of how both we and the

02:48

coding assistant validate a feature. It's how we close a

02:53

single PIV loop.

02:54

And all 5 of these levels here should look very

02:57

familiar to you because we have this as a part

03:00

of our structured plan. And you built your own structured

03:03

plan in planning command in the last exercises.

03:06

And so if you're missing any of these levels, this

03:07

is an opportunity for you to evolve your system and

03:10

add these.

03:11

And so we start with level 1, syntax and style.

03:15

This is for the coding assistant to run to really

03:17

just tidy up our code base.

03:19

And so doing things like code formatting

03:21

and, following linting rules. And the tool that we use

03:25

is specific to your tech stack and the programming language,

03:27

which is 1 of the reasons why all the commands

03:30

I have as the resources for you here is just

03:32

a starting point. This is the 1 of the things

03:33

you might have to customize.

03:36

And so rough for Python,

03:37

we can use,

03:39

ESLint or Prettier for TypeScript. I'm gonna keep using Python

03:42

and TypeScript as the examples because those are the 2

03:45

primary languages that I use.

03:47

And then you go to level 2 type safety. Type

03:50

checking and static analysis

03:51

like my py for Python or you can use,

03:55

well, TypeScript is just built right into the programming language.

03:57

So just making sure that types are good for our

04:00

function parameters and variables and things like that. And then

04:03

we go on to unit testing. And again, the AI

04:06

coding assistant is doing all of these things. It's handling

04:08

this as a part of our structure plan. So like

04:11

pytest for Python,

04:13

jest or vtest for typescript.

04:15

This is where we're testing the most granular little pieces

04:18

of code at a time that we can. So generally

04:21

for unit tests, you'll create quite a few of them,

04:24

maybe even dozens of them for a single feature so

04:27

that you can test edge cases and things like that.

04:30

And then level 4 gets into integration tests. This is

04:33

where we test the system or the new feature as

04:36

a whole, and this is where your coding assistant can

04:39

actually start the application. And then for example, interact with

04:42

an API endpoint through curl. It can visit a website

04:45

with the Playwright MCP server which I'll show you later

04:48

in the course. And so doing live validation

04:51

on the application that is actually running. And then you

04:54

can also use standard testing libraries for end to end

04:56

testing as well. It's not always running the application but

04:59

a lot of it can be And this is 1

05:01

of the really cool things that AI coding assistance checking

05:03

its own work when it's actually interacting with the application

05:06

similar to how a user would. And then last but

05:09

not least, we have human review. I haven't showed it

05:12

that much in the last couple of modules because I

05:14

wanna talk about validation more here, but it is really

05:17

important to review the code. Don't just trust the coding

05:21

assistant blindly. Remember, trust but verify.

05:25

And so, yeah, you can ask it questions here. Maybe

05:27

if you're not as experienced with coding, like, just make

05:29

sure that you are on the same page with what

05:31

was actually built here. And it's also your job to

05:34

perform the strategic alignment check. Does the implementation really match

05:38

with my vision and the structure plan that I already

05:41

went through and validated?

05:43

So that is the 5 levels with the coding assistant

05:46

mostly handling levels 1 through 4. And all of this

05:49

is embedded in our structured plan. And as a part

05:52

of this validation, you can also guide the coding assistant

05:55

on CLI tools to use or MCP servers like Playwright

05:59

that I mentioned earlier, and we'll get into that later

06:01

in the course.

06:02

And then the last thing that I wanna say here

06:04

is that sometimes you also wanna run the validation outside

06:07

of an embedded plan. And so we can take the

06:09

5 levels

06:11

and isolate that into a slash command. This is an

06:14

example that I have for you right here for the

06:16

Obsidian agents. You can take this and tweak it to

06:18

your project. Because sometimes you want to iterate, you know,

06:22

fix a couple of bugs with the coding assistant and

06:24

then run validate again without having it be a part

06:26

of the structure command. So I'll show you an example

06:29

of that to make it more concrete

06:31

at the end here after we do our little pivot

06:34

loop focusing on validation. And last but not least, validation

06:38

as feedback.

06:39

When the validation fails,

06:41

it reveals a problem with our system most of the

06:44

time. I mean, sometimes AI coding assistance just mess up

06:47

this tiny little thing and we'll fix it and be

06:48

done. But if you recognize a pattern of something specific

06:51

failing a few times or more than a few times,

06:55

It's an opportunity to go and look, should I update

06:57

my rules, my on demand context, should I update my

06:59

commands? That's another thing that we'll get into at the

07:02

end of this module

07:04

when we cover leveraging

07:06

the system review command and creating that as 1 of

07:09

our exercises.

07:10

So when you see the same validation failures repeatedly, that

07:13

is a signal to improve your system. And so with

07:17

that, let's get into a PIV loop. Let's see these

07:20

5 layers in action. So back over to our Obsidian

07:22

agents to go through the PIV loop again, focusing on

07:25

validation this time. We've never done that before. So I

07:28

am gonna go pretty quickly through everything else so that

07:31

we get to the validation pyramid.

07:33

So remember, don't try this at home. I'm going a

07:35

lot faster than I normally would. I mean, you can

07:38

definitely speed up over time as you evolve your system,

07:40

but it still is gonna be an exaggeration

07:42

of what I would typically do even though I'm gonna

07:45

be building our third tool into this agent, and I've

07:47

already built 2. So you can go fast, probably not

07:50

this fast. So I already did my priming here. I

07:53

got it comfortable with the code base. And then,

07:56

classic question, based on the PRD, what should we build

07:59

for our agent?

08:01

And remember, we have 3 tools that we defined when

08:03

we created our PRD,

08:05

and we already built 2 of them in the last

08:07

module. The 1 to query our vault and all the

08:10

search capabilities,

08:11

and then our vault manager for things like notes and

08:14

folders.

08:15

And so the last tool that we had here is

08:17

more of workflow oriented reading with contextual information, like reading

08:21

multiple notes and making distinctions between them, that kind of

08:25

thing. And, honestly, this is pretty vague. I'm not entirely

08:28

sure what it's going to build here, but I left

08:30

it like that because I am genuinely very curious. And

08:33

so, yeah, sure enough, it suggested exactly what I hoped

08:36

it would. You have 2 of 3 tools implemented. You

08:39

don't have Obsidian get context. And so its recommendation for

08:42

me right now is to build the Obsidian get context

08:45

tools into the agent. That is perfect. So I'm gonna

08:48

go forward with this. So mostly skipping by planning here,

08:51

but I will at least add a little bit of

08:53

context as I create the structured plan. So I'm gonna

08:55

call in plan dash feature, which I haven't moved into

08:58

the core Pivot Loop folder because I did organize things

09:01

in this repository just like the core 1 for our

09:03

core. So if you go to the commands here, you

09:05

can see the core PIV loop and then, like, the

09:07

validation ones that we'll cover throughout this module. So I

09:10

got that all nice and tidy for you. And then

09:11

I'll go into Aquavoice and say, yes. Let's go ahead

09:14

and implement the Obsidian and get context as the next

09:17

feature for our agent. Now these tools, there's definitely a

09:21

big risk of over engineering them when we have workflow

09:24

based tools. So I want you to keep things very

09:26

simple here. Just a few different capabilities with not a

09:30

ton of logic to help me, you know, cross reference

09:33

different files and read things in batch and gain insight

09:36

from multiple files. I think that would be very useful

09:38

but, yeah. Help me think through this and let's make

09:41

it simple here. Alright. Send that in. I'll go ahead

09:43

and send it off to create the structured plan, and

09:46

I'll come back once that is done. Alright. Our structured

09:49

plan is complete. Let's take a look. I'm actually really

09:51

curious the capabilities that it decided for the get context

09:55

tool. And the beauty of the structured plan is we

09:58

know exactly where to look. We have our solution statement.

10:01

And so taking a look at this, here are the

10:03

5 context types. And if I don't agree with any

10:06

of this, I can just revise my plan here. So

10:09

I gave quite a bit up to the coding assistant

10:11

to figure out because I was really curious what capabilities

10:14

you wanted to add, but I could always change this

10:16

as a part of the planning. And so it's going

10:18

to have a tool to read a single note, read

10:20

multiple, gather related

10:22

daily notes, and then notes with backlinks, which these last

10:25

2 in particular, it very much understands things that are

10:28

specific to Obsidian, which I really like about this because

10:31

daily notes is something that's built right in. And then

10:33

backlinks as well as a way to connect different notes

10:37

together. So very, very cool.

10:39

And the problem that we have here is that through

10:41

the manager tool, we can modify tools but we can't

10:43

actually read the full content. So it's definitely something that

10:46

it recognizes missing in the existing tools that we have

10:49

for agents. So thinking about what capabilities should we add.

10:52

So I love its thought process here. And another quick

10:55

tip is with the planning command that I showed you

10:58

in this course, we have a confidence that it creates

11:01

at the end of creating the structured plan. And so

11:03

validation can apply to a structured planning as well, not

11:06

just implementation, though that's what I'm gonna focus on here.

11:09

So 1 thing we can do is say, why is

11:11

the confidence

11:13

a 9 out of 10?

11:16

What would you need

11:18

to make it a 10 out of 10? And so

11:21

iterating on the structure plan not just by injecting your

11:24

own thoughts but by asking it, like, what else are

11:27

we missing here in our vibe planning? What are we

11:30

missing in the research that you did for the planning?

11:32

What can we do to make this a 10 out

11:33

of 10? So get to the point where you're confident.

11:35

And then, yeah, what I wanna focus on here is

11:37

our 5 layers of planning. So let's scroll down because

11:39

this is always at the end of our structured plans,

11:42

and let's take a look at this. Because this is

11:43

what we are always doing at the very end of

11:46

a feature implementation. So here we go. We're creating our

11:49

test package, test fixtures. This is for the unit testing

11:53

specifically. That's what we are outlining right here. And then

11:56

we have our integration testing. Very, very neat. Just using

11:59

pytest more for that. And then we have our linting

12:02

and type checking. So we're using pytest, my py, pywrite,

12:05

and rough. Really, really nice. All the tools that I

12:07

called out when I was going over the validation pyramid

12:09

at the start of this video. And then we also

12:11

have our manual validation,

12:13

which this is a part of integration testing as well.

12:16

It's all the end to end testing when the agent

12:19

is going to start our application and interact with it

12:22

through some requests to the API. So this is looking

12:26

phenomenal. Next, now that I'm happy with my plan, I'm

12:28

gonna go in and I'm gonna run the commit. So

12:30

let's just go ahead and create a safe state where

12:32

we're at so that if there's any mistakes that are

12:34

drastic enough, we can just revert right back to having

12:37

our structure plan and nothing implemented and retry.

12:40

Alright. Our commit is done and the status is ready

12:43

to implement. So like usual, we're gonna start our execution

12:46

in a brand new context window. So slash clear or

12:49

start a new conversation and whatever your AI coding assistant

12:52

is. Then let's do slash execute.

12:54

Execute.

12:55

There we go. And then I will copy the path

12:58

to our new tool plan. So copy relative path, paste

13:02

it in. Boom. Send it in. And so I'm going

13:04

to pause but not come back when it's done like

13:07

usual. I'm gonna come back once we get to the

13:10

validation pyramid, and then I'll kinda just show things live,

13:13

just to give you a little bit of a demonstration.

13:15

Maybe you won't watch the entire thing, but I want

13:17

you to see what it looks like as the coding

13:20

assistant is checking its own work. Okay. So about 5

13:22

minutes later and it has gotten through the entire implementation

13:26

now onto the testing.

13:28

And the very first thing that we saw in our

13:29

structured plan was creating those test fixtures, and sure enough

13:33

that is what it is doing here. So creating everything

13:36

to begin building the unit tests. So fixtures are working.

13:40

Now it's creating the unit tests. It'll do that, then

13:43

get to the integration test. It's just doing things in

13:45

the order that we had in the structured plan. So

13:48

I'll go back to that here. We're doing everything that

13:50

we're looking at right here first. Now typically, I actually

13:53

like doing the linting and type checking first. That's why

13:56

I have it as layer 1 and 2 in the

13:57

validation pyramid. So honestly, that's probably something I would improve

14:01

with this structured plan here. So there we go. Another

14:05

opportunity to evolve our system, being more clear in our

14:08

planning command that we do really wanna follow the layers

14:11

in order for our different testing. But this works fine

14:14

overall, so I'm not too worried about it. Let's go

14:16

back in and see exactly what it's doing here. Alright.

14:19

So it's still working on creating the tools here or

14:21

the testing. Well, actually, it ran some of the tests.

14:23

So it it built some of the tests and run

14:25

them. Ran them, it, found some issues, and so it's

14:29

addressing them. So very good. The AI coding assistant is

14:32

already iterating on its implementation,

14:34

thanks to the very first set of unit tests that

14:37

it created. Okay. Now onto the integration test. So all

14:40

the unit tests were passing just after a few minutes

14:43

here. Now it's writing our end to end tests, and

14:45

you're gonna see a lot of failures

14:47

as you're watching the validation run from executing a structured

14:50

plan. That's totally okay. There's always gonna be these little

14:53

things that we have for the linting and just import

14:57

errors, all this random stuff that it has to clean

14:59

up. That's why we're doing this. And so don't be

15:01

alarmed when you see it having to correct itself quite

15:03

a bit because it's fixing even the super tiny stuff.

15:07

Alright. Looking good. So we have some errors in our

15:10

unit integration test, but it's just because I'm not running

15:12

Postgres right now. I don't have that Docker container running,

15:14

and it recognized that, which is good. So it's flexible.

15:17

It's not like hard set on fixing literally everything when

15:20

it's not related to what I'm doing right now. So

15:22

that's good. And now it's getting on to using my

15:24

py and pywrite. And so, yeah, we're gonna be doing

15:26

our linting and all of our type checking now, which

15:30

this is especially where you'll see a lot of errors

15:32

because linting and type checking

15:34

can get extremely

15:36

nitpicky,

15:37

but it's not like it actually is making big mistakes

15:40

or anything. So just let it go through all this.

15:42

Sometimes it'll take extra long, but it's always worth it

15:46

just to keep your code base very nice and tidy

15:48

and easy to maintain. Alright. Take a look at this.

15:51

We've finished everything except for the manual testing now. And

15:54

so it spun up the server on its own, the

15:56

API endpoint with the agent behind it. And now it's

15:59

trying to test out some of the new tools. And

16:01

so first it tried reading a file. So it asked

16:05

the agent to use a tool to read a specific

16:07

file, but that file didn't exist. It passed in something

16:10

that isn't actually in our Obsidian vault, but it recognized

16:12

that and now it's iterating. Right? And so validation is

16:16

all about it trying things, hitting mistakes, finding errors in

16:19

the test or the code whatever, and then just fixing

16:21

itself, getting to the point where it has the full

16:24

test completed end to end. So there we go. The

16:26

agent correctly used the Obsidian get context tool with the

16:30

read note and the target of the intro course script.

16:34

And alright. Wow. This is just awesome. So without us

16:37

even having to do the manual testing, which we'll get

16:39

into soon, it is able to verify things fully end

16:43

to end for us. This is the power of the

16:46

validation pyramid. So at this point, the AI validation

16:50

has completed. We went through the first 4 layers of

16:53

the validation pyramid. And so now it is up to

16:55

us to do our manual testing. And so this includes

16:59

everything including linting

17:01

and type checking. I just didn't list that here explicitly

17:04

to make it concise. And these are the 2 main

17:06

things. But this is all done. And so now it

17:08

is our job for human validation. So I wanna talk

17:10

a little bit about code reviews and manual testing. And

17:14

the reason I don't wanna hit on this too much

17:16

right now is because this varies so much project to

17:20

project. The things that you do specifically for reviewing code

17:23

and then especially your manual testing. And so I'll give

17:26

some general recommendations, but, yeah, we'll start with manual testing.

17:29

And for this, we really just act as a regular

17:32

user going through our application, trying out the new feature.

17:35

And so let's open up Copilot.

17:37

I have the agent running in the terminal here.

17:40

I'll send in a prompt that I already have ready

17:42

to paste in. Find the 3 personal AI automations you

17:45

need to know, so I'm calling out this fake YouTube

17:47

script specifically and then asking it to summarize everything after

17:50

reading the full content. And reading the whole content is

17:53

1 of those new tools that we just added. And

17:56

so looking at the logs here, there is quite a

17:58

bit that I had to do. There's a lot of

17:59

tool calls that I had to leverage just to perform

18:02

this workflow for me basically.

18:04

And, yeah, this is a really awesome summary. I mean,

18:07

it basically just laid out the entire

18:09

script for me because it's so short, but, yeah, it

18:11

did its job. This is phenomenal. So that's really it

18:14

for manual testing.

18:16

And I could do quite a bit more trying all

18:18

the different capabilities we added, like finding related notes and

18:21

daily notes and things like that, but I think you

18:23

get the point for what you're supposed to do here

18:25

based on your application. And then we have code reviewing.

18:28

For code reviewing,

18:30

you don't have to look at every single line of

18:33

code that is outputted, especially as you have been evolving

18:36

your systems and get to the point where you really

18:38

trust them. You don't have to do too much of

18:40

this.

18:41

And when we really get into automating our systems

18:44

in the next few modules, we're gonna be doing less

18:47

and less of code review because we're trusting our coding

18:50

assistant more and more. That's the whole point of systematizing

18:54

things.

18:55

But even if it just means like asking the coding

18:57

assistant questions,

18:58

like, did you consider security?

19:00

Or what are the new environment variables we have to

19:03

add? Is there anything different with our setup? What did

19:05

you do for your testing exactly? Like, all these different

19:07

things you can ask just as a way to not

19:09

have to look at as much code. And then like

19:11

I've said a couple of times before, if you're not

19:13

as comfortable with coding or your code base, that's also

19:16

a way to not have to understand the code extremely

19:19

deeply, but still be able to validate your coding assistant.

19:21

And so 2 strategies for code review that I wanna

19:24

cover really quickly,

19:25

just at a high level. First is I love looking

19:28

at the git diffs. This is why we're using git

19:30

as a core part of our system, at least 1

19:32

of the reasons. We can look at the files that

19:34

were changed. I know this is kinda hard to read,

19:36

but I'm just trying to show us at a high

19:37

level. We can look at the things that were created.

19:40

We wanna make sure things aren't over engineered. We wanna

19:42

at least analyze the code at a high level to

19:44

understand what was implemented and so I'm using GitHub Desktop

19:47

as a tool for this. Another thing you can do

19:50

is just go into your coding assistant and say,

19:53

give me the git diff.

19:56

Just tell me the files that were changed and created.

20:00

And so this is just another way to not have

20:02

to go into another application, but get that high level

20:04

overview of what was changed and then you can go

20:06

into those files individually

20:07

and do the same kind of validation.

20:09

And boom. There we go. 8 files created, 7 were

20:12

modified. I know that I covered the strategy already a

20:15

bit in module 2, but I'm still just trying to

20:17

be really comprehensive here as we're going through the validation

20:20

pyramid. And then, yeah, last I just wanna show a

20:22

random example of like, what did you do for security

20:25

here? Right? Like, you can just ask it questions to

20:28

have it guide you through the review itself. Good. So

20:30

it definitely considered security, just giving me a summary of

20:33

how it did so.

20:35

And we're gonna get into the code review command next

20:38

so we can really start to systematize this review, to

20:41

have the coding assistant walk us through things, do part

20:44

of the review itself. We still wanna be a part

20:45

of the process, but we do want to build it

20:47

into our system. So that's just a sneak peek for

20:50

what we have coming up in the next video, which

20:52

is gonna be an exercise for you to think about

20:54

how you'd want to have the coding assistant

20:57

do a review on the code outside of the validation

20:59

itself, but high level looking over the changes like we're

21:02

doing right now manually.

21:04

And so with that, the last thing that I wanna

21:06

cover here is I wanna talk about the validation command,

21:09

which I have that in this repository in the validation

21:12

folder. And then also within the primary repository for this

21:17

course, I have that in the commands validation folder right

21:20

here. So link to this below

21:22

along with all these other commands as a resource for

21:24

you.

21:25

Because the thing with this command

21:27

is it really is just doing part of what we

21:29

have in the structure to plan. But like I said

21:32

at the start of this video, sometimes you want to

21:34

do validation outside of being in the middle of executing

21:38

a structured plan. Like, right, you might iterate a couple

21:40

of times fixing some bugs, and then you want to

21:42

rerun

21:43

everything in the validation pyramid. And so we go through

21:46

our test suite. We do our type checking with mypy

21:49

and pywrite, our linting with rough. Again, this is specific

21:52

to Python and our code base, but it is a

21:54

good place to work off of as a starting command.

21:56

And then we run the application, do a curl to

21:59

the API endpoint, doing our manual end to end testing.

22:02

So we're doing all this and then we're having it

22:04

output a summary report because remember, we always want some

22:07

kind of output formatting at the end of every command

22:09

thinking about who is going to be consuming the output

22:12

of this command. In this case, it will probably be

22:14

us so that we can verify

22:16

all the validation ourself afterwards. And so I'll just show

22:19

you what this looks like. I'll just clear the conversation

22:22

again, or I guess what I should do right now

22:24

is do a commit. So I'll commit first to really

22:26

close things off,

22:28

then I will go and run the validate just as

22:30

another example for you. I gotta be honest, I have

22:33

forgotten to commit a couple of times in the pivot

22:35

loops in the last couple of modules, but it is

22:37

really important to do that after your plan, after your

22:40

implementation, and validation.

22:42

And we did this. I my, camera is blocking it

22:45

here, but we have 4 percent left before our auto

22:48

compact. And so just in time we have finished with

22:51

our implementation and validation.

22:53

And it's not the end of the world when you

22:55

hit an auto compact with your AI coding assistant, but

22:57

it is nice to avoid that. And part of our

22:59

system

23:00

is to make things concise,

23:02

have different context windows for planning and execution to avoid

23:05

those things. And so now I'm going to slash clear

23:08

new conversation

23:09

and let's go ahead and just run the slash validate

23:12

here. And so this is very simply just going to

23:15

go through all of the layers of the validation

23:18

pyramid. And take a look at this, even though we've

23:20

been doing validation as a part of every PIV loop

23:23

all along the way, we still have some type safety

23:26

and code formatting issues that we found

23:29

when we did the highest level validation possible because sometimes

23:32

coding assistance gets so stuck in the weeds when they're

23:35

implementing a specific feature that they affect things outside of

23:39

their current feature implementation and they don't realize it. And

23:42

so it is helpful to do this kind of validation

23:45

at a high level once in a while. And it

23:47

is just very little things and all the tests are

23:50

passing, but, yeah, I do want to address number 2

23:52

and 3. So I'll say address number 2 and number

23:55

3. The whole database not running. I mean, that's fine.

23:57

I specifically don't have it running right now. So I'm

24:00

just gonna have it address number 2 and 3, and

24:02

I'll come back once that's done. And there we go.

24:05

Our validation is now in tip top shape. The only

24:07

thing we have left is just those issues, so I'm

24:09

not actually running Postgres right now. But we are good

24:12

to go. So I'm gonna run a commit again. We

24:14

got a really good save state here, and that's everything

24:16

that I have for the validation pyramid.

24:19

And so going back to our

24:21

diagram here, we have covered everything for the AI validation

24:26

and everything for the human validation.

24:28

That is within a single PIV loop.

24:31

Now the rest of this module on systems for validation

24:35

is actually kind of going beyond an individual PIV loop.

24:39

How can we do entire code reviews on our code

24:41

base? How can we review our system

24:44

as it relates to different different feature implementations and evolve

24:47

our system with the help of our coding assistant? And

24:50

so that takes us into the next exercise

24:52

where I'll guide you through creating your own code review

24:55

command as a part of your validation system.