---
course: Dynamous Agentic Coding
module: 7
lesson: "7.4"
title: "Exercise - Create Your System Review Command"
type: exercise
status: raw
date_added: 2026-02-18
date_modified: 2026-02-18
tags:
  - dynamous
  - agentic-coding
  - module-7-validation
  - exercise
  - system-review-command
key_concepts: []
prerequisites: ["7.1", "7.2", "7.3"]
related_lessons: ["8.1"]
---

Now you've made it to the second exercise for creating

00:03

your systems for validation. And this, my friend, is where

00:06

it starts to get really fun because we're gonna go

00:09

through creating the part of our system

00:11

for evolving our system. I know that's very meta, but

00:14

this is some powerful stuff. And this exercise is going

00:17

to be a little bit different than the other ones

00:19

because there are some prerequisites that I wanna guide you

00:21

through before I send you off to do the exercise.

00:24

And so this is gonna be kind of a mix

00:26

of a guide and an exercise for you. So in

00:29

the last couple of videos, we covered code review.

00:32

That is finding bugs in the current implementation in a

00:35

single PIV loop.

00:37

But what we're doing here, and honestly, I think this

00:39

is a lot higher leverage,

00:41

is with a system review, we are finding bugs in

00:44

our AI coding workflow,

00:46

finding issues in our system. And what I mean by

00:49

that is we're gonna create a process

00:52

that analyzes a PIV loop to find discrepancies

00:55

between our structured plan and what was actually implemented because

00:59

any discrepancy there, any issue in the implementation

01:03

is a chance for us to go back, think through

01:05

what should we fix in our system so that doesn't

01:08

happen again. And we're creating the commands here to make

01:10

that process very seamless. And this is the most complex

01:14

part of our validation system. And I'm not saying it's

01:17

always critical for you to run a system review, but

01:20

man, does this unlock so much power for you. So

01:23

I really want you to go through this exercise.

01:26

What we are focusing on here is divergence

01:29

analysis.

01:30

So reviewing our system to figure out why our implementation

01:34

diverges from a structured plan in 1 or more PIV

01:38

loops.

01:39

And not all divergence is bad, but you are going

01:42

to get it most of the time. No matter how

01:44

evolved your system is, there are going to be little

01:47

discrepancies that come up between your structured plan and what

01:50

is actually implemented.

01:51

And sometimes that is good, like your plan assumes something

01:55

that didn't exist or the coding assistant realizes there's a

01:57

better way to do something when it's getting into the

02:00

building and diving into your code more. So it isn't

02:02

always bad,

02:03

but probably a majority of the time it is. Your

02:06

coding assistant ignores constraints in the plan. It creates new

02:10

architecture instead of following existing, so it's ignoring layer 1

02:13

planning.

02:14

This kind of stuff introduces a lot of technical debt.

02:17

Part of our system review is not necessarily finding issues

02:20

in our code. I mean, that's more what our code

02:22

review and validation does, but it's more about finding these

02:26

inconsistencies

02:27

introduced in our code as we are going through PIV

02:30

loop after PIV loop. That might make it just a

02:32

lot harder for us and our AI coding assistant to

02:35

maintain our code base and build on top of it

02:37

going forward.

02:38

We always want our code base to be extremely consistent

02:41

with our architecture and tech stack and the way that

02:44

we're testing and logging.

02:46

We need to maintain all of that very neatly. And

02:49

so we wanna document

02:50

both types of diversions.

02:52

And before we create our command

02:55

to perform the system review, that's gonna be your exercise.

02:58

Here's the main prerequisite.

03:00

We need to create what is called the execution

03:02

report. So the execution report is a markdown document that

03:06

you create right after implementation

03:08

when you want to use this PIV loop to go

03:11

into system review. So not every pivot loop you have

03:14

to do system review, but when you want, right after

03:17

implementation in the same context window, you are going to

03:20

run this execution report command

03:23

to generate a report. Before you do any validation yourself,

03:26

before you commit anything, you're gonna run this because you

03:29

want to work on the raw state of the code

03:32

right after implementation.

03:34

Because that's really what we wanna compare directly against our

03:37

structured plan before we even have a chance to iterate

03:40

and share our own thoughts with the coding assistant.

03:43

And so this command, like all the other ones, I

03:46

have it in the dot agents slash commands folder for

03:48

you. I have it right here in validation. So execution

03:51

report, and then we have the system review that you'll

03:54

create your exercise in a bit here.

03:56

And so going to the Obsidian agent, I have it

03:59

loaded up here as well.

04:01

Execution

04:02

dash report. And so what I want to do

04:05

is I'm gonna first clear here. I wanna go back

04:08

into the conversation

04:09

that I had with Cloud Code when I was in

04:12

the implementation

04:13

for the third tool we added to our Obsidian Agent.

04:15

That's the last full PIV loop we did at the

04:17

start of this module. And so no matter AI coding

04:20

assistant, there's always gonna be some way to go back

04:22

to a previous conversation. So I'm gonna do slash resume,

04:26

and then I'm going to find my conversation

04:29

where I did the implementation.

04:31

And, yes, it looks like this is the 1 right

04:33

here. So this is the plan, planning the third MVP

04:36

tool, and then I cleared and went into the implementation.

04:39

This is a conversation we want to load. So even

04:41

though I did quite a bit of work since then,

04:44

working on some of the validation stuff that I was

04:46

showing you, I can still go back to this conversation,

04:49

at the point where we made this commit here.

04:52

And so I know this isn't exactly how I just

04:54

said you should do it. Like, typically, you should run

04:57

the report

04:58

right after implementation before you iterate and commit,

05:01

but that's just the flow of the course that I

05:03

have for you right now. So I have to go

05:04

back to an old conversation. And so now I can

05:07

do slash execution

05:09

report. Boom. And it's going to run this workflow, which

05:12

I can show this again really quickly.

05:14

Just a little bit of context. You've just finished implementing

05:16

a feature. Before moving on, reflect on what you implemented,

05:20

how it aligns with the plan. And it knows the

05:22

plan because that's the argument we sent into the execute

05:24

command that's at the top of the current context. And

05:27

then we're going to save the report out to a

05:29

markdown document because that is what we're going to feed

05:32

into the system review command. So take note of that

05:35

because this is 1 of the inputs for the command

05:37

that you'll create in the exercise. We need to input

05:40

the execution report.

05:42

And so plan file, files added, files modified.

05:46

Very, very good. And just has to, like, you know,

05:48

come up with these values itself. Validation results, what went

05:51

well, challenges encountered. And so I'm not gonna cover this

05:53

in insane amount of detail here. But, yeah, basically, it's

05:55

just like reflect on how the implementation went for you

05:58

compared to the plan, generate a report. And now that's

06:01

gonna be our context for the full review. And there

06:04

we go. We have our execution report. And I looked

06:08

at this off camera camera really quickly,

06:09

and it thought that things went extremely well. So going

06:12

down to the bottom here, the overall assessment is that

06:15

it was excellent. And then scrolling back up to the

06:18

top, it said there was no significant diverges from the

06:21

plan, which

06:23

we don't really care about what it thinks here. The

06:25

system review command is when we really wanna dive into

06:28

the structure plan and compare against this report.

06:31

And so you can kinda think of it like the

06:33

execution report command being the student,

06:36

and the system review is the instructor. And the instructor

06:39

asks the student, how do you think you did? And

06:42

so our execution report will make a claim for how

06:45

it thinks it did. But when we get into the

06:47

system review with the command you'll create, that's when we

06:49

really do the analysis. And so for the most part

06:51

with this document, I care about just outlining the challenges

06:55

that were encountered

06:56

and what went well. I just want it to reflect

06:59

and then use that as context going into the full

07:01

system review. Now we come to the part that is

07:04

your job. We have our execution report. Now we wanna

07:07

build the command for system review that will analyze an

07:10

execution report against the original plan. And so if you

07:13

haven't done so yet, I would highly encourage you to

07:16

take this execution report, maybe tweak the command a bit

07:19

to your own project, and then run it right after

07:22

implementation in a PIV loop so that you have an

07:24

execution report to validate against your system review command when

07:29

you create it for this exercise.

07:31

And we have done quite a few exercises in the

07:34

course up until this point. And so for this 1,

07:37

I'm gonna give a little bit less instruction.

07:39

Just a couple of things for you to think about,

07:41

and then I'm giving it as a challenge for you

07:43

too. With only this, I want you to try and

07:46

create the system review command completely from scratch. Of course,

07:50

using AI coding assistant to help if you want. Now

07:53

if you're not extremely comfortable doing that, it is okay

07:56

if you wanna review the solution to guide you in

07:58

creating your own customized system review command. So in the

08:02

folder for this video, this exercise, I have the solution,

08:05

which also is exactly the same as the system dash

08:08

review command that I have in the primary set of

08:11

commands as the resource for you. So you can take

08:12

a look at that if you want for inspiration, but

08:14

I do challenge you to try to just think about

08:17

these things and make it for yourself.

08:19

So think about what context does the agent need. Like,

08:23

maybe you want to include the commands that you use

08:26

for planning and executing. I'm not saying you have to.

08:28

It's just a couple of ideas for you because maybe

08:31

you want it to analyze not just the plan, but

08:33

the process that led to creating the plan and turning

08:36

that into code. And of course, you probably want to

08:39

give it the structure plan as some input and the

08:42

execution report. So think about what you want. Think about

08:45

what kind of analysis it should perform.

08:47

Think about what it should output. Something like an alignment

08:50

score, improvement actions. And is this gonna be output for

08:53

us to consume as humans? So should it be shorter,

08:56

more structured,

08:57

or should it be longer and more detailed because it's

08:59

output for another coding assistant to maybe action upon this

09:04

system review? So think about those things. There's no right

09:07

answer here. Just like pretty much all of these exercises.

09:10

Go ahead create the command, and then you can even

09:13

test it as a part of your exercise. So once

09:15

you have the report

09:17

generated, then you can run the system review. And for

09:20

example, you might have 2 arguments for the command. I'm

09:22

not saying this is exactly right, but you might have

09:24

an argument for the path to the plan and then

09:27

the path to the execution report. So, yeah, go ahead,

09:30

knock out the creation of the system review command, and

09:32

when you come back, I will go through my solution

09:35

and we'll see it in action. Welcome back. Now we

09:37

will dive into my solution for the system review, and

09:40

we'll see it in action. So I'll go ahead and

09:42

open that up right now. So I kick things off

09:45

right away by being super clear with the coding assistant

09:48

that system review is not code review. The last thing

09:52

I wanted to do is basically just do what the

09:54

code review or the code review fix commands do. We're

09:57

not looking for bugs in the code, we're looking for

09:59

bugs in the process.

10:00

And so it is analyzing

10:02

for plan adherence and diversions patterns.

10:05

And so for the input, the first section of every

10:08

command, there are 4 things that I'm giving it because

10:11

I really want it to understand

10:13

the entire process of the PIV loop here. So our

10:16

plan command, just calling out the path for it to

10:18

read here. The execute command, another path. And we could

10:21

have the at symbol in front of these as well

10:23

because we definitely want it to read it no matter

10:25

what, but this is definitely going to work as well.

10:28

And then for the arguments to our command,

10:31

we have the path to the generated plan, the structure

10:33

plan, and then the path to the execution report that

10:36

we just generated with the previous command. And so that

10:40

is all of our input, and now we have our

10:41

process, our workflow.

10:43

So understand the planned approach, we want it to read

10:47

the structure plan in a lot of detail, and then

10:49

understand the implementation. So these are the 2 main things

10:52

that we're going to compare, also looking at the commands

10:54

for the process. And then for each of the diversion

10:57

set it finds, we wanted to label it as good

10:59

or bad. Either 1. We want to document both and

11:02

then trace the root cause of all of these divergences.

11:06

Was the plan unclear?

11:08

I. E. We have to change something with our structure

11:10

plan or our planning command. Was context missing? Like, should

11:13

we add some on demand context? Maybe we need to

11:16

add something to our global rules. Was validation missing? Was

11:19

there something that we should change in our execute command,

11:21

for example? Like, Like, what is the problem? And then

11:24

it will generate process improvements based on the pattern. What

11:28

should we change within our layer 1 and layer 2

11:31

planning, or should we update something in our commands? That

11:33

is what we want the guidance from with the coding

11:36

assistant. We want it to help us figure out these

11:38

things so that we can use it to evolve our

11:41

system. And then we have our output format here. So

11:44

it's going to output its final review with its suggestions

11:48

as well. Like, which 1 of these things should we

11:50

update? Like, our plan command or our global rules. And

11:54

so we'll have recommendations for each of those. So let's

11:57

go and run a system review now. And I'm gonna

11:59

do it live with you. I haven't done this yet.

12:02

And that really is the case for all of the

12:04

exercises and demos because I want it to be really

12:06

authentic. So I have no idea what it's gonna say

12:09

is wrong with the system, but whatever it does, maybe

12:11

I'll pick out a couple of things and work on

12:13

that so I can show you evolving the system live

12:16

as well. So I will do slash system

12:19

review and that is within our validation folder. And then

12:23

there are 2 arguments that we have here. First, we

12:26

have the path to our plan and that is this

12:29

1 right here. So I will right click. I will

12:32

copy the relative path. This is the plan that we

12:34

generated in our last PIV loop. And then the second

12:37

argument is the path to our execution report that we

12:40

just generated together in this video. So right click, copy

12:43

relative path, paste it in, and boom, there we go.

12:47

And then, we have the paths just hard coded in

12:49

for the 2 commands that we wanted to look at.

12:52

And so it's gonna go ahead and find those and

12:54

use that as the 4 pieces of context. So I

12:57

will pause and come back once it is done with

13:00

the full review. Alright. So our system review command has

13:03

come back with key findings and then some recommendations for

13:06

ways we can evolve our system. And the overall alignment

13:10

score is a 9.5

13:11

out of 10. Looking really, really good. It was a

13:14

near perfect implementation

13:16

with 100 percent planned adherence and 0 significant divergence. So

13:20

I guess a pat on the shoulder for that. Our

13:22

system is obviously working extremely well through the entire PIV

13:25

loop. That is the goal. And so it's very fortunate,

13:29

but a little unfortunate at the same time because there's

13:31

nothing super obvious that jumps out for like, okay, we

13:34

should evolve our system in this way. If my system

13:37

wasn't as well planned out or if I was implementing

13:40

more complex features, there would definitely be some recommended actions

13:44

that would jump out a lot more at me here.

13:46

So looking at all of these suggestions here, they're decent,

13:49

but I wanna be very picky. What I actually action

13:53

on it to update my system. Because for example, adding

13:56

all these little random 1 off things to my global

13:58

rules is just gonna end up bloating it over time.

14:02

So the main warning that I have for you here

14:05

when you run your system review is that LLMs and

14:08

AI coding assistants,

14:10

they like to over engineer. So they're always gonna recommend

14:12

these tiny little things to update that we really don't

14:14

actually care about, and so you should be really selective

14:18

based on the recommendations here. The things that you think

14:20

are going to actually apply

14:22

to PIV loops and feature implementations going forward.

14:26

Like this whole time zone aware daytime usage thing, like,

14:29

I really don't think that's a high leverage thing I

14:32

want to include in my global rules. Some of these

14:34

though, like the vault path calculation standard, yeah, actually, I

14:37

probably will. Or maybe like the

14:40

import order warning and the integration phase, integration test pattern

14:44

specification, like, yeah, those are actually important for the testing

14:46

part,

14:48

of the structure plan that we create. So, yeah, some

14:50

things I want to include. So I'm gonna go through

14:53

into

14:53

my Aquavoice here and just list out the things I

14:57

want it to help us evolve our system with. Okay.

14:59

This looks good overall. Definitely more recommendations here than I

15:03

actually care to implement. So for updating the claw dot

15:06

MD,

15:07

let's include the tool registry import order pattern

15:11

and the integration test pattern for tools. Those 2 are

15:14

good. And then for the plan feature,

15:16

let's do the last 2. Import order warning and integration

15:19

phase and the integration test pattern specification.

15:22

And then finally for the execute command,

15:25

let's do the test first approach note. I want to

15:28

include that. Everything else I don't really think is important

15:30

to include to evolve our system.

15:34

And boom. There we go. And you could create a

15:36

whole command around actioning upon these recommendations if you want

15:39

as well. I'm just gonna do this a bit ad

15:41

hoc right now, but there is always an opportunity to

15:44

create commands and systematize pretty much everything

15:47

that you're doing with an AI coding assistant. And so

15:50

you could take this further and command this. You could

15:53

make it so that,

15:55

maybe it recommends less things overall. Like, honestly, I probably

15:58

could improve my system review command

16:01

so that it

16:03

doesn't recommend as much junk. But LLMs just will always

16:06

do that by nature. And so even when I try

16:07

to make it so that the structured plans aren't as

16:10

long or it doesn't recommend as many things, it's always

16:12

going to do a bit more than I want. So

16:14

that's why I have that warning for you. So, yeah,

16:16

I'll come back once we have our evolved system. And

16:19

there we go. The changes are complete and our system

16:23

is evolved. And because this is such a high leverage

16:26

task that could potentially affect all PIV loops going forward,

16:30

please

16:31

please make sure you validate everything that it changed with

16:33

your commands and your layer 1 and layer 2 planning.

16:36

It is so crucial.

16:38

And so you can validate just like you would after

16:40

the implementation. So go into GitHub desktop or just ask

16:43

it for a list of changes and look at those

16:45

files in your IDE.

16:46

Totally up to you. So I have it super zoomed

16:48

in here so you can see my GitHub desktop. We

16:51

can take a look at the diffs directly.

16:53

And so I'm not gonna show everything here, but I

16:55

wanna point out something really cool in the global rules.

16:58

So it made a change here to help with our

17:01

integration testing. And I know this is really specific and

17:04

technical, but I really appreciate that it says right here,

17:07

test the service layer for our tools directly, not the

17:10

tool

17:11

registration function. Here is what we should be doing. Here

17:14

is what we should not be doing.

17:16

And okay. Just take a minute with me to appreciate

17:20

the beauty of this. We went through the implementation on

17:23

a structured plan.

17:24

The validation told us what we needed to do during

17:27

implementation

17:28

to run our integration test. So we created it, we

17:31

ran it, and we encountered some issues.

17:33

And then when we created our execution report, because that

17:36

was in the same context where we ran into those

17:39

problems,

17:39

that ended up in our execution report. We ran into

17:42

these issues doing integration testing on the tools. The the

17:46

execution report goes into the system review, and now that

17:49

ends up as a recommendation that we add into our

17:51

global rules so that we evolved it. And now all

17:54

integration testing going forward for our agent tools is going

17:58

to be that much smoother. Like, this is just incredible.

18:00

I know it's super specific,

18:02

but I just wanted to give you 1 example of

18:04

how with our layer 1 planning, with our global rules,

18:07

we create it once and update rarely. We don't change

18:09

it much, but doing the system review, that is when

18:12

you change these things. Even your rules that you barely

18:15

want to update, we will evolve them once in a

18:18

while, and that is what we're doing right here. And

18:20

that, my friend, is the end of module 7 and

18:22

really the end of the second chapter of the Dynamis

18:25

agentic coding course. So if you've made it this far,

18:28

congratulations.

18:29

You now have a system in place for reliable and

18:32

repeatable coding with AI coding assistance.

18:35

And if you went through all the exercises and videos,

18:38

you really can consider yourself the master of the PIV

18:42

loop now. But it doesn't end here because now we

18:45

move on to the third chapter or the third release

18:48

batch of the course. For this batch, we're gonna get

18:50

into some more advanced topics

18:52

like sub agents, for example. I'll talk about MCP servers

18:55

and tools that we can give our coding assistance to

18:58

make our whole system more powerful.

19:00

And then we'll get into automating our systems,

19:03

doing things in GitHub, remote agent decoding. So we can

19:06

code from anywhere, from our phone, from our platforms like

19:09

Slack and Telegram. This is where it gets really exciting.

19:11

So we have this resource that I'm gonna be creating

19:14

for you as a part of this course that allows

19:17

you to code remotely and inject your own systems that

19:20

we've been creating as we've gone through the PIV loop

19:23

time and time again and have been evolving things. And

19:26

I've got the rough outline for the next release batch

19:29

up right here so you get an idea of what

19:31

is coming.

19:32

Really excited for this. We're just gonna continue to take

19:34

things further and further as we evolve our system and

19:37

trust it more and more. And so I will see

19:39

you in the next modules to push the limits even

19:42

further.