---
course: Dynamous Agentic Coding
module: 1
lesson: "1.3"
title: "Introduction to the System Gap"
type: lecture
status: raw
date_added: 2026-02-18
date_modified: 2026-02-18
tags:
  - dynamous
  - agentic-coding
  - module-1-introduction
  - system-gap
key_concepts: []
prerequisites: ["1.1", "1.2"]
related_lessons: ["1.5", "2.1"]
---


So the big question that you might have going into

00:03

everything that I have for you is why do we

00:06

even care about building a system to work with AI

00:09

coding assistance? Is it really worth all that extra engineering

00:13

to put that in place? Now obviously, the short answer

00:16

is yes, but I don't wanna just dive right into

00:18

the how with you. I want to really introduce you

00:21

to why we care about building a system

00:23

starting with the term that I have coined the system

00:26

gap. This is what I use to describe

00:29

this huge discrepancy we have between people that don't have

00:32

a system in place and people who do. Because there

00:36

are 2 camps in the world right now. It's very

00:38

polarized. Some people think that AI coding assistants are awful,

00:42

and they hallucinate way too much. And then other people

00:45

like me and hopefully like you very soon use AI

00:49

coding assistance as a daily driver no matter what we

00:52

want to build. So the big question we have is

00:55

how can AI coding assistance be simultaneously

00:58

extremely capable

01:00

and so disappointing for so many developers?

01:03

That is the system gap. I wanna talk about this

01:06

for a few minutes with you here. So let me

01:07

start with a fictitious

01:09

example here, something that definitely does happen in the real

01:12

world to show you what happens when we don't have

01:14

a system.

01:15

So we have this customer or a development lead, whoever's

01:18

giving the request to us or the developer in this

01:21

example. And they say, build filter functionality

01:24

into our product catalog, which by the way, this is

01:27

going to be the first exercise for you in this

01:30

course. And they give this request to 2 different developers.

01:34

Both are using the exact

01:36

same tool, but developer 1 does not have a system.

01:40

They basically just take the request that their development lead

01:43

sent in Slack, for example, and throw it right into

01:46

cloud code. Developer 2, on the other hand, does have

01:49

a system. So here is what happens. It's not quite

01:52

what you think because I'm gonna say for this example

01:55

that it actually does end up working for developer number

01:59

1. Because this is a rather simple request overall, an

02:02

AI coding assistant can definitely knock this out of the

02:05

park. However, it takes 30 minutes, and the key thing

02:08

is the code is over engineered. AI coding assistants love

02:12

to produce more code than you actually need, and that

02:14

makes it hard to validate and maintain things.

02:18

There's no testing, there's no documentation,

02:20

and the UI is pretty awkward because LLMs will mess

02:24

that up quite a bit unless you're very clear for

02:26

them. So that's the results for developer 1 with no

02:29

system. Now developer 2, they do have a full workflow

02:33

in place for planning, then implementing, then validating. They got

02:37

their commands and rules. All these things that we'll talk

02:39

about in the next couple of modules. It, of course,

02:42

works and it only takes 10 minutes.

02:44

The code is simple. We got full testing and documentation.

02:48

The user interface looks perfect because it's exactly what they

02:51

specify.

02:52

The important thing here is not just that it works

02:54

because honestly, it did for both implementations.

02:57

But the main thing is that everything fit exactly to

03:01

the developer's specification.

03:02

That's 1 of the golden rules that we have here

03:05

is when we build a system and we build these

03:08

workflows, we want to decrease

03:10

the number of assumptions the coding assistant has to make

03:13

as much as possible because that is what leads to

03:16

these great results that we have with developer 2. So

03:20

the difference is a setup and a skill gap. It

03:22

is the system gap that I'm trying to drive home

03:25

here. Developer 2 invested the time in setting up their

03:29

AI for success with a repeatable

03:31

and reliable workflow. That is the gap. Developer 1 had

03:35

nothing like this. Now, unfortunately,

03:37

even though this was a fictitious

03:39

example that I covered,

03:41

a lot of people are like developer number 1. They

03:44

don't have a system in place and they get poor

03:46

results because of that. And that leads to the paradox

03:49

that I wanna cover with you right now.

03:51

According to a Google Dora survey, and this was actually

03:54

crazy to me when I first read the statistic,

03:56

90 percent of people building software are already relying on

04:00

AI coding assistance. This number is insane.

04:03

However, 46 percent of these people actively

04:07

distrust the accuracy

04:08

according to a Stack Overflow survey. So we have this

04:12

crazy disconnect here where everyone is starting to rely on

04:14

coding assistance because people know that this is the future

04:17

of software development. But at the same time, they're very

04:20

distrustful because they don't have a process in place to

04:23

actually get reliable and repeatable results.

04:26

So the average code acceptance

04:28

for anyone who doesn't have a system is around 30

04:31

percent.

04:32

So of the AI generated code, 70 percent of it

04:35

is just scrapped or has to be refined quite a

04:38

bit. But for the top users

04:40

that have a system in place for working with coding

04:43

assistance, you can get up to 88 percent code acceptance.

04:47

And, of course, it's never gonna be perfect, but it's

04:50

still pretty close. And this is a really attractive a

04:52

number, especially because you can always keep validating and iterating

04:55

until you make your code perfect.

04:57

And so how is this paradox possible?

05:00

Well, like you're kind of already realizing here, I'm sure,

05:03

it's never a problem with the AI coding assistant. It's

05:07

a problem with how you use the tool, AKA

05:10

your system.

05:12

And that goes back to what I was saying earlier.

05:14

When your AI coding assistant makes a mistake, it is

05:16

an opportunity for you to correct the system and retry.

05:20

So every mistake

05:22

is a chance for you to evolve your process.

05:25

And sure, sometimes LLMs are imperfect and they make crazy

05:28

mistakes even in a perfect system. But it's not productive

05:32

for you or me to blame the LLM and not

05:35

try to fix our system. So I just wanna get

05:38

you in this very productive mindset. Whenever there's a mistake,

05:41

let's fix our system. That's what I'm gonna be showing

05:44

you how to do in this course. Build the system,

05:47

evolve the system. Most people dive right into implementation.

05:50

They don't understand the tools or how to set up

05:52

their AI for success. Top performers

05:55

understand that it is a very worthy time investment,

05:59

and I want that to be you. And the good

06:01

news is that all of these skills

06:03

are things you can learn. That's what I'm gonna teach

06:05

you. I'm gonna show you how to get into the

06:07

top 5 percent and well beyond that. So with that,

06:10

the next video is going to get into the first

06:13

exercise that I have for you. I'm really excited for

06:16

you to take this on. It's an exercise to help

06:19

you establish your baseline, to help you think about

06:22

where you might be in the system gap. And maybe

06:25

you're brand new to AI coding and you realize that

06:27

you have to learn all of these things, building workflows,

06:29

even using, like, slash commands and rules and how to

06:31

set those up. Maybe you're more advanced and you actually

06:34

already have a process in place, or you're using a

06:37

strategy like PRP or BMAD. There's still a ton for

06:40

you to get out of this course to help you

06:42

evolve your workflow as well, but I want you to

06:44

figure out where you are at establishing

06:46

your baseline. So using a real task as the example

06:50

here,

06:51

I want you to

06:53

build out the implementation in the exercise using your prompts,

06:56

your process, no restrictions or guidance. It's not a test.

07:00

It is a self assessment. I want you to track

07:03

a few things, especially your confidence level. How much you

07:06

feel like you're in the driver's seat when doing the

07:08

implementation.

07:09

Because it is a simpler 1, it'll probably work even

07:12

with the basic prompt, but I want you to think

07:14

about how much control you really have there. And so

07:17

the game plan is after this exercise,

07:19

we'll cover the mental models

07:21

around the PIV loop. I'll introduce you to that in

07:23

more detail, Then you get to tackle the exercise another

07:27

time and refine your approach based on what we cover.

07:29

That'll be exercise number 2.

07:31

Then we'll have a live workshop after that, which maybe

07:34

already happens. You can go check that out and really

07:36

see my solution.

07:38

And then we'll get into how we can systemize the

07:40

approach and covering a lot of those advanced techniques for

07:44

context engineering and covering things like MCP servers and sub

07:47

agents and remote coding and parallel agent decoding. We'll get

07:50

into all of that. And so yeah, we got a

07:53

lot in store here. So with that, on to the

07:55

next video for the exercise.